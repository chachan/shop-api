"""
This type stub file was generated by pyright.
"""

import struct
from bson import CodecOptions
from bson.son import SON
from typing import Any, Optional

"""Tools for creating `messages
<http://www.mongodb.org/display/DOCS/Mongo+Wire+Protocol>`_ to be sent to
MongoDB.

.. note:: This module is for internal use and is generally not needed by
   application developers.
"""
MAX_INT32 = 2147483647
MIN_INT32 = - 2147483648
_COMMAND_OVERHEAD = 16382
_INSERT = 0
_UPDATE = 1
_DELETE = 2
_EMPTY = b''
_BSONOBJ = b'\x03'
_ZERO_8 = b'\x00'
_ZERO_16 = b'\x00\x00'
_ZERO_32 = b'\x00\x00\x00\x00'
_ZERO_64 = b'\x00\x00\x00\x00\x00\x00\x00\x00'
_SKIPLIM = b'\x00\x00\x00\x00\xff\xff\xff\xff'
_OP_MAP = { _INSERT: b'\x04documents\x00\x00\x00\x00\x00',_UPDATE: b'\x04updates\x00\x00\x00\x00\x00',_DELETE: b'\x04deletes\x00\x00\x00\x00\x00' }
_FIELD_MAP = { 'insert': 'documents','update': 'updates','delete': 'deletes' }
_UJOIN = u"%s.%s"
_UNICODE_REPLACE_CODEC_OPTIONS = CodecOptions(unicode_decode_error_handler='replace')
def _randint():
    """Generate a pseudo random 32 bit integer."""
    ...

def _maybe_add_read_preference(spec, read_preference):
    """Add $readPreference to spec when appropriate."""
    ...

def _convert_exception(exception):
    """Convert an Exception into a failure document for publishing."""
    ...

def _convert_write_result(operation, command, result):
    """Convert a legacy write result to write commmand format."""
    ...

_OPTIONS = SON([('tailable', 2), ('oplogReplay', 8), ('noCursorTimeout', 16), ('awaitData', 32), ('allowPartialResults', 128)])
_MODIFIERS = SON([('$query', 'filter'), ('$orderby', 'sort'), ('$hint', 'hint'), ('$comment', 'comment'), ('$maxScan', 'maxScan'), ('$maxTimeMS', 'maxTimeMS'), ('$max', 'max'), ('$min', 'min'), ('$returnKey', 'returnKey'), ('$showRecordId', 'showRecordId'), ('$showDiskLoc', 'showRecordId'), ('$snapshot', 'snapshot')])
def _gen_find_command(coll, spec, projection, skip, limit, batch_size, options, read_concern, collation: Optional[Any] = ..., session: Optional[Any] = ...):
    """Generate a find command document."""
    ...

def _gen_get_more_command(cursor_id, coll, batch_size, max_await_time_ms):
    """Generate a getMore command document."""
    ...

class _Query(object):
    """A query operation."""
    __slots__ = ...
    exhaust_mgr = ...
    cursor_id = ...
    def __init__(self, flags, db, coll, ntoskip, spec, fields, codec_options, read_preference, limit, batch_size, read_concern, collation, session, client):
        self.flags = ...
        self.db = ...
        self.coll = ...
        self.ntoskip = ...
        self.spec = ...
        self.fields = ...
        self.codec_options = ...
        self.read_preference = ...
        self.read_concern = ...
        self.limit = ...
        self.batch_size = ...
        self.collation = ...
        self.session = ...
        self.client = ...
        self.name = ...
    
    def namespace(self):
        ...
    
    def use_command(self, sock_info, exhaust):
        ...
    
    def as_command(self, sock_info):
        """Return a find command document for this query."""
        ...
    
    def get_message(self, set_slave_ok, sock_info, use_cmd: bool = ...):
        """Get a query message, possibly setting the slaveOk bit."""
        ...
    


class _GetMore(object):
    """A getmore operation."""
    __slots__ = ...
    name = ...
    def __init__(self, db, coll, ntoreturn, cursor_id, codec_options, read_preference, session, client, max_await_time_ms, exhaust_mgr):
        self.db = ...
        self.coll = ...
        self.ntoreturn = ...
        self.cursor_id = ...
        self.codec_options = ...
        self.read_preference = ...
        self.session = ...
        self.client = ...
        self.max_await_time_ms = ...
        self.exhaust_mgr = ...
    
    def namespace(self):
        ...
    
    def use_command(self, sock_info, exhaust):
        ...
    
    def as_command(self, sock_info):
        """Return a getMore command document for this query."""
        ...
    
    def get_message(self, dummy0, sock_info, use_cmd: bool = ...):
        """Get a getmore message."""
        ...
    


class _RawBatchQuery(_Query):
    def use_command(self, socket_info, exhaust):
        ...
    
    def get_message(self, set_slave_ok, sock_info, use_cmd: bool = ...):
        ...
    


class _RawBatchGetMore(_GetMore):
    def use_command(self, socket_info, exhaust):
        ...
    
    def get_message(self, set_slave_ok, sock_info, use_cmd: bool = ...):
        ...
    


class _CursorAddress(tuple):
    """The server address (host, port) of a cursor, with namespace property."""
    def __new__(cls, address, namespace):
        ...
    
    @property
    def namespace(self):
        """The namespace this cursor."""
        ...
    
    def __hash__(self):
        ...
    
    def __eq__(self, other):
        ...
    
    def __ne__(self, other):
        ...
    


_pack_compression_header = struct.Struct("<iiiiiiB").pack
_COMPRESSION_HEADER_SIZE = 25
def _compress(operation, data, ctx):
    """Takes message data, compresses it, and adds an OP_COMPRESSED header."""
    ...

def __last_error(namespace, args):
    """Data to send to do a lastError.
    """
    ...

_pack_header = struct.Struct("<iiii").pack
def __pack_message(operation, data):
    """Takes message data and adds a message header based on the operation.

    Returns the resultant message string.
    """
    ...

_pack_int = struct.Struct("<i").pack
def _insert(collection_name, docs, check_keys, flags, opts):
    """Get an OP_INSERT message"""
    ...

def _insert_compressed(collection_name, docs, check_keys, continue_on_error, opts, ctx):
    """Internal compressed unacknowledged insert message helper."""
    ...

def _insert_uncompressed(collection_name, docs, check_keys, safe, last_error_args, continue_on_error, opts):
    """Internal insert message helper."""
    ...

if _use_c:
    _insert_uncompressed = _cmessage._insert_message
def insert(collection_name, docs, check_keys, safe, last_error_args, continue_on_error, opts, ctx: Optional[Any] = ...):
    """Get an **insert** message."""
    ...

def _update(collection_name, upsert, multi, spec, doc, check_keys, opts):
    """Get an OP_UPDATE message."""
    ...

def _update_compressed(collection_name, upsert, multi, spec, doc, check_keys, opts, ctx):
    """Internal compressed unacknowledged update message helper."""
    ...

def _update_uncompressed(collection_name, upsert, multi, spec, doc, safe, last_error_args, check_keys, opts):
    """Internal update message helper."""
    ...

if _use_c:
    _update_uncompressed = _cmessage._update_message
def update(collection_name, upsert, multi, spec, doc, safe, last_error_args, check_keys, opts, ctx: Optional[Any] = ...):
    """Get an **update** message."""
    ...

_pack_op_msg_flags_type = struct.Struct("<IB").pack
_pack_byte = struct.Struct("<B").pack
def _op_msg_no_header(flags, command, identifier, docs, check_keys, opts):
    """Get a OP_MSG message.

    Note: this method handles multiple documents in a type one payload but
    it does not perform batch splitting and the total message size is
    only checked *after* generating the entire message.
    """
    ...

def _op_msg_compressed(flags, command, identifier, docs, check_keys, opts, ctx):
    """Internal OP_MSG message helper."""
    ...

def _op_msg_uncompressed(flags, command, identifier, docs, check_keys, opts):
    """Internal compressed OP_MSG message helper."""
    ...

if _use_c:
    _op_msg_uncompressed = _cmessage._op_msg
def _op_msg(flags, command, dbname, read_preference, slave_ok, check_keys, opts, ctx: Optional[Any] = ...):
    """Get a OP_MSG message."""
    ...

def _query(options, collection_name, num_to_skip, num_to_return, query, field_selector, opts, check_keys):
    """Get an OP_QUERY message."""
    ...

def _query_compressed(options, collection_name, num_to_skip, num_to_return, query, field_selector, opts, check_keys: bool = ..., ctx: Optional[Any] = ...):
    """Internal compressed query message helper."""
    ...

def _query_uncompressed(options, collection_name, num_to_skip, num_to_return, query, field_selector, opts, check_keys: bool = ...):
    """Internal query message helper."""
    ...

if _use_c:
    _query_uncompressed = _cmessage._query_message
def query(options, collection_name, num_to_skip, num_to_return, query, field_selector, opts, check_keys: bool = ..., ctx: Optional[Any] = ...):
    """Get a **query** message."""
    ...

_pack_long_long = struct.Struct("<q").pack
def _get_more(collection_name, num_to_return, cursor_id):
    """Get an OP_GET_MORE message."""
    ...

def _get_more_compressed(collection_name, num_to_return, cursor_id, ctx):
    """Internal compressed getMore message helper."""
    ...

def _get_more_uncompressed(collection_name, num_to_return, cursor_id):
    """Internal getMore message helper."""
    ...

if _use_c:
    _get_more_uncompressed = _cmessage._get_more_message
def get_more(collection_name, num_to_return, cursor_id, ctx: Optional[Any] = ...):
    """Get a **getMore** message."""
    ...

def _delete(collection_name, spec, opts, flags):
    """Get an OP_DELETE message."""
    ...

def _delete_compressed(collection_name, spec, opts, flags, ctx):
    """Internal compressed unacknowledged delete message helper."""
    ...

def _delete_uncompressed(collection_name, spec, safe, last_error_args, opts, flags=...):
    """Internal delete message helper."""
    ...

def delete(collection_name, spec, safe, last_error_args, opts, flags=..., ctx: Optional[Any] = ...):
    """Get a **delete** message.

    `opts` is a CodecOptions. `flags` is a bit vector that may contain
    the SingleRemove flag or not:

    http://docs.mongodb.org/meta-driver/latest/legacy/mongodb-wire-protocol/#op-delete
    """
    ...

def kill_cursors(cursor_ids):
    """Get a **killCursors** message.
    """
    ...

class _BulkWriteContext(object):
    """A wrapper around SocketInfo for use with write splitting functions."""
    __slots__ = ...
    def __init__(self, database_name, command, sock_info, operation_id, listeners, session, op_type, codec):
        self.db_name = ...
        self.command = ...
        self.sock_info = ...
        self.op_id = ...
        self.listeners = ...
        self.publish = ...
        self.name = ...
        self.field = ...
        self.start_time = ...
        self.session = ...
        self.compress = ...
        self.op_type = ...
        self.codec = ...
    
    def _batch_command(self, docs):
        ...
    
    def execute(self, docs, client):
        ...
    
    def execute_unack(self, docs, client):
        ...
    
    @property
    def check_keys(self):
        """Should we check keys for this operation type?"""
        ...
    
    @property
    def max_bson_size(self):
        """A proxy for SockInfo.max_bson_size."""
        ...
    
    @property
    def max_message_size(self):
        """A proxy for SockInfo.max_message_size."""
        ...
    
    @property
    def max_write_batch_size(self):
        """A proxy for SockInfo.max_write_batch_size."""
        ...
    
    @property
    def max_split_size(self):
        """The maximum size of a BSON command before batch splitting."""
        ...
    
    def legacy_bulk_insert(self, request_id, msg, max_doc_size, acknowledged, docs, compress):
        ...
    
    def legacy_write(self, request_id, msg, max_doc_size, acknowledged, docs):
        """A proxy for SocketInfo.legacy_write that handles event publishing.
        """
        ...
    
    def write_command(self, request_id, msg, docs):
        """A proxy for SocketInfo.write_command that handles event publishing.
        """
        ...
    
    def _start(self, request_id, docs):
        """Publish a CommandStartedEvent."""
        ...
    
    def _succeed(self, request_id, reply, duration):
        """Publish a CommandSucceededEvent."""
        ...
    
    def _fail(self, request_id, failure, duration):
        """Publish a CommandFailedEvent."""
        ...
    


_MAX_SPLIT_SIZE_ENC = 2097152
class _EncryptedBulkWriteContext(_BulkWriteContext):
    __slots__ = ...
    def _batch_command(self, docs):
        ...
    
    def execute(self, docs, client):
        ...
    
    def execute_unack(self, docs, client):
        ...
    
    @property
    def max_split_size(self):
        """Reduce the batch splitting size."""
        ...
    


def _raise_document_too_large(operation, doc_size, max_size):
    """Internal helper for raising DocumentTooLarge."""
    ...

def _do_batched_insert(collection_name, docs, check_keys, safe, last_error_args, continue_on_error, opts, ctx):
    """Insert `docs` using multiple batches.
    """
    ...

if _use_c:
    _do_batched_insert = _cmessage._do_batched_insert
_OP_MSG_MAP = { _INSERT: b'documents\x00',_UPDATE: b'updates\x00',_DELETE: b'deletes\x00' }
def _batched_op_msg_impl(operation, command, docs, check_keys, ack, opts, ctx, buf):
    """Create a batched OP_MSG write."""
    ...

def _encode_batched_op_msg(operation, command, docs, check_keys, ack, opts, ctx):
    """Encode the next batched insert, update, or delete operation
    as OP_MSG.
    """
    ...

if _use_c:
    _encode_batched_op_msg = _cmessage._encode_batched_op_msg
def _batched_op_msg_compressed(operation, command, docs, check_keys, ack, opts, ctx):
    """Create the next batched insert, update, or delete operation
    with OP_MSG, compressed.
    """
    ...

def _batched_op_msg(operation, command, docs, check_keys, ack, opts, ctx):
    """OP_MSG implementation entry point."""
    ...

if _use_c:
    _batched_op_msg = _cmessage._batched_op_msg
def _do_batched_op_msg(namespace, operation, command, docs, check_keys, opts, ctx):
    """Create the next batched insert, update, or delete operation
    using OP_MSG.
    """
    ...

def _batched_write_command_compressed(namespace, operation, command, docs, check_keys, opts, ctx):
    """Create the next batched insert, update, or delete command, compressed.
    """
    ...

def _encode_batched_write_command(namespace, operation, command, docs, check_keys, opts, ctx):
    """Encode the next batched insert, update, or delete command.
    """
    ...

if _use_c:
    _encode_batched_write_command = _cmessage._encode_batched_write_command
def _batched_write_command(namespace, operation, command, docs, check_keys, opts, ctx):
    """Create the next batched insert, update, or delete command.
    """
    ...

if _use_c:
    _batched_write_command = _cmessage._batched_write_command
def _do_batched_write_command(namespace, operation, command, docs, check_keys, opts, ctx):
    """Batched write commands entry point."""
    ...

def _do_bulk_write_command(namespace, operation, command, docs, check_keys, opts, ctx):
    """Bulk write commands entry point."""
    ...

def _batched_write_command_impl(namespace, operation, command, docs, check_keys, opts, ctx, buf):
    """Create a batched OP_QUERY write command."""
    ...

class _OpReply(object):
    """A MongoDB OP_REPLY response message."""
    __slots__ = ...
    UNPACK_FROM = ...
    OP_CODE = ...
    def __init__(self, flags, cursor_id, number_returned, documents):
        self.flags = ...
        self.cursor_id = ...
        self.number_returned = ...
        self.documents = ...
    
    def raw_response(self, cursor_id: Optional[Any] = ...):
        """Check the response header from the database, without decoding BSON.

        Check the response for errors and unpack.

        Can raise CursorNotFound, NotMasterError, ExecutionTimeout, or
        OperationFailure.

        :Parameters:
          - `cursor_id` (optional): cursor_id we sent to get this response -
            used for raising an informative exception when we get cursor id not
            valid at server response.
        """
        ...
    
    def unpack_response(self, cursor_id: Optional[Any] = ..., codec_options=..., user_fields: Optional[Any] = ..., legacy_response: bool = ...):
        """Unpack a response from the database and decode the BSON document(s).

        Check the response for errors and unpack, returning a dictionary
        containing the response data.

        Can raise CursorNotFound, NotMasterError, ExecutionTimeout, or
        OperationFailure.

        :Parameters:
          - `cursor_id` (optional): cursor_id we sent to get this response -
            used for raising an informative exception when we get cursor id not
            valid at server response
          - `codec_options` (optional): an instance of
            :class:`~bson.codec_options.CodecOptions`
        """
        ...
    
    def command_response(self):
        """Unpack a command response."""
        ...
    
    def raw_command_response(self):
        """Return the bytes of the command response."""
        ...
    
    @classmethod
    def unpack(cls, msg):
        """Construct an _OpReply from raw bytes."""
        ...
    


class _OpMsg(object):
    """A MongoDB OP_MSG response message."""
    __slots__ = ...
    UNPACK_FROM = ...
    OP_CODE = ...
    def __init__(self, flags, payload_document):
        self.flags = ...
        self.payload_document = ...
    
    def raw_response(self, cursor_id: Optional[Any] = ...):
        ...
    
    def unpack_response(self, cursor_id: Optional[Any] = ..., codec_options=..., user_fields: Optional[Any] = ..., legacy_response: bool = ...):
        """Unpack a OP_MSG command response.

        :Parameters:
          - `cursor_id` (optional): Ignored, for compatibility with _OpReply.
          - `codec_options` (optional): an instance of
            :class:`~bson.codec_options.CodecOptions`
        """
        ...
    
    def command_response(self):
        """Unpack a command response."""
        ...
    
    def raw_command_response(self):
        """Return the bytes of the command response."""
        ...
    
    @classmethod
    def unpack(cls, msg):
        """Construct an _OpMsg from raw bytes."""
        ...
    


_UNPACK_REPLY = { _OpReply.OP_CODE: _OpReply.unpack,_OpMsg.OP_CODE: _OpMsg.unpack }
def _first_batch(sock_info, db, coll, query, ntoreturn, slave_ok, codec_options, read_preference, cmd, listeners):
    """Simple query helper for retrieving a first (and possibly only) batch."""
    ...

